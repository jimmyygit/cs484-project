{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtAsYkGZm_o5"
      },
      "source": [
        "# Object Removal and Impainting using Generative Adverserial Network\n",
        "Project by: Anna Wang, Jimmy Yang and Philip Chen\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "\n",
        "This project aims to explore the applications of using a Generative Adverserial Network (GANs) to remove objects and impaint over specific regions in images. The art of object impainting is a technique used to seamlessly insert or remove objects within an image, often used for image editing or restoration. Given a masked area, we wish to fill in the masked-area with new content that blends realistically with the rest of the image.\n",
        "\n",
        "Object removal and image inpainting are inherently challenging tasks in computer vision. To help us tackle this project, we employ the use of a special variant of GANs, a Deep Convolutional Generative Adversarial Network (DCGANs). With the help of convolutional layers for image processing, we architected a generator to produce convincing image completions, while the discriminator learns to distringuish between authentic images and the generator's images. As the two models compete in an adversarial process, they both improve their respective abilities, leading to a better trained generator to inpaint masked images.\n",
        "\n",
        "To evaluate performance, a curated dataset of cat images was used, with manually masked holes of the same size inserted in the middle of all images. This choice of dataset allows for testing the model's ability to handle fine textures like fur and preserve anatomical consistency. The generator performs reasonably well at reconstructing altered parts of the cats body, but struggles when it comes to facial features.\n",
        "\n",
        "Ultimately, this project highlighted the complexity of the inpainting task. We conclude with several observations and potential directions for improving the model's performance in future work."
      ],
      "metadata": {
        "id": "OyuVt2sFz02E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Team Members and Contributions\n",
        "\n",
        "- Anna Wang ( , @uwaterloo.ca):\n",
        "\n",
        "- Jimmy Yang (20890430, jj7yang@uwaterloo.ca): I helped curate the dataset of cats to be used in model training. I worked with Anna to split our original dataset into images of cats and defined a custom CatDataset class to help us insert square masks for training purposes. I also explored various model training methods against our dataset to help decide what image resolution to use and how to architect our models. Then, together with the group I contributed to fine-tuning our models, analyzing our results, and iterating on improvements.\n",
        "\n",
        "- Philip Chen ( , @uwaterloo.ca):"
      ],
      "metadata": {
        "id": "KJKDsDfR7Kse"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVlIQt3em_o7"
      },
      "source": [
        "# 1. Code Libraries\n",
        "\n",
        "The following external Python libraries were essential to the development and implementation of this project:\n",
        "\n",
        "- NumPy:\n",
        "Used for efficient numerical operations and array manipulation throughout the project, particularly when handling image masks, pixel data, and preprocessing pipelines.\n",
        "\n",
        "- Matplotlib:\n",
        "Used for visualizing training progress, loss curves, and before-and-after comparisons of image inpainting results. It plays a key role in debugging and showcasing the model's performance.\n",
        "\n",
        "- PyTorch:\n",
        "Foundational deep learning framework for this project. It enabled the construction and training of the GAN architecture through its neural network modules and optimization tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GuG0iKQim_o7"
      },
      "outputs": [],
      "source": [
        "# Standard libraries for math and plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Standard PyTorch for network training\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# Other utilities and misc\n",
        "import os\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymj9A-sRAFf2",
        "outputId": "c1115def-b2c3-495e-8479-e84de696dc53"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vkRGg_tm_o8"
      },
      "source": [
        "# 2. Importing Dataset\n",
        "\n",
        "Our dataset comes from Hugging Face Dataset cats_vs_dogs: https://huggingface.co/datasets/microsoft/cats_vs_dogs. From the dataset, we filter for images of cats and split up training, validation, and testing sets.\n",
        "\n",
        "We install the standard `datasets` library to use the dataset in our training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heX7mrWwm_o8",
        "outputId": "4b6fc30a-b636-4c11-aa7a-dc4f6beae71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# uncomment cli login if need to login\n",
        "# !huggingface-cli login\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHecz7sFm_o9"
      },
      "source": [
        "### 2.1 Importing Cats vs. Dogs Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugOo3g40m_o9",
        "outputId": "b9d990b6-a152-4001-e99a-3fe34cf53823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['image', 'labels'],\n",
            "    num_rows: 23410\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"microsoft/cats_vs_dogs\")\n",
        "\n",
        "print(ds['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjxdv9m2m_o9"
      },
      "source": [
        "### 2.2 Filtering Dataset by Cats and Dogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piBCg-Ztm_o-",
        "outputId": "724d8331-1d54-4918-87ed-57c48adacfea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 23410\n",
            "Total dataset size: 23410\n",
            "Dogs: 11669\n",
            "Cats: 11741\n"
          ]
        }
      ],
      "source": [
        "# filtering data by cats and dogs\n",
        "data = ds['train']\n",
        "dogs = data.filter(lambda isDog: isDog['labels'] == 1)\n",
        "cats = data.filter(lambda isDog: isDog['labels'] == 0)\n",
        "print(\"Dataset size: \" + str(len(data)))\n",
        "\n",
        "print(\"Total dataset size:\", len(data))\n",
        "print(\"Dogs:\", len(dogs))\n",
        "print(\"Cats:\", len(cats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTVofi3mm_o-"
      },
      "source": [
        "### 2.3 Splitting each class into training, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKuDfzI0m_o-"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "train_test_split = cats.train_test_split(test_size=0.2, seed=42)\n",
        "test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
        "train_dataset = train_test_split['train']['image']\n",
        "val_dataset = test_val_split['train']['image']\n",
        "test_dataset = test_val_split['test']['image']\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBjuWowhm_o_"
      },
      "source": [
        "# 3. Creating Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw6jyp7Qm_o_"
      },
      "source": [
        "### 3.1 Masks\n",
        "\n",
        "In order to create training samples for our model, we create the function `create_masks` that cuts out the center of an image of size `hole_size`. We acknowledge that designing a network model for inpainting has a number of challanges. To simplify the project, we assume that the holes are always square, and that they are always in the center and have the same size. The central region is surrounded by image context on all four sides—top, bottom, left, and right. This gives the model rich, balanced context to infer what the missing content should look like. During training, consistently masking the center allows the model to focus its capacity on learning to inpaint a specific region. This makes convergence faster and often leads to better initial performance compared to random or irregular masking. Future improvements to the model would be to allow for custom created masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8In7wH0Wm_o_"
      },
      "outputs": [],
      "source": [
        "def create_masks(num_masks):\n",
        "    hole_size = 20\n",
        "    im_size = 64\n",
        "\n",
        "    x = int((im_size - hole_size) / 2.0)\n",
        "    y = int((im_size - hole_size) / 2.0)\n",
        "    mask = torch.zeros((1, im_size, im_size))\n",
        "    mask[0, y : y + hole_size, x : x + hole_size] = 1\n",
        "    masks = mask.repeat_interleave(num_masks, dim=0)\n",
        "\n",
        "    return masks.unsqueeze(1)\n",
        "\n",
        "plt.imshow(create_masks(1)[0][0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcS-FGRlm_o_"
      },
      "source": [
        "### 3.2 Creating Cat Dataset Definition\n",
        "\n",
        "The following class defines our Cat Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXhD2Azpm_o_"
      },
      "outputs": [],
      "source": [
        "class CatDataset(Dataset):\n",
        "    def __init__(self, all_imgs, transforms=None) -> None:\n",
        "        super().__init__()\n",
        "        self.transforms = transforms\n",
        "        self.all_imgs = all_imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.all_imgs[index]\n",
        "\n",
        "        # 1. Transform the image as needed\n",
        "        transformed_img = self.transforms(image)\n",
        "\n",
        "        if transformed_img.shape[0] == 1:\n",
        "            transformed_img = transformed_img.repeat(3, 1, 1)\n",
        "\n",
        "        # 2. Create copy of image before masking\n",
        "        ground_truth_image = deepcopy(transformed_img)\n",
        "\n",
        "        # 3. Create center mask depending of transformed image\n",
        "        mask = create_masks(1)[0]\n",
        "        transformed_img = (1 - mask) * transformed_img\n",
        "\n",
        "        return transformed_img, ground_truth_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGx0AOx0m_o_"
      },
      "source": [
        "### 3.3 Transformations\n",
        "\n",
        "While composing our dataset, we experiments with several transformations, including what image resolution to train on, various crops, and flips. We landed on the resolution of 64x64 because it would allow the images to retain just enough detail to make out facial features (ie. eyes, nose, mouth), as opposed to smaller resolutions like 32x32 that would make the images almost unrecognizable as cats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlFXZC0rm_o_"
      },
      "outputs": [],
      "source": [
        "transforms = v2.Compose([\n",
        "    v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "    v2.Resize(size=(64, 64))\n",
        "])\n",
        "\n",
        "transformed_train_dataset = CatDataset(train_dataset, transforms=transforms)\n",
        "transformed_val_dataset = CatDataset(val_dataset, transforms=transforms)\n",
        "transformed_test_dataset = CatDataset(test_dataset, transforms=transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phlyWEdm_pA"
      },
      "source": [
        "### 3.4 Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENsG_FmRm_pA"
      },
      "outputs": [],
      "source": [
        "# Grab pictures of just cats, some other pictures have their owners in the background\n",
        "image0, image1, image2 = transformed_train_dataset[1], transformed_train_dataset[3], transformed_train_dataset[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEppQnYdm_pA"
      },
      "outputs": [],
      "source": [
        "# Display training images and their ground truth images\n",
        "f, ax = plt.subplots(3, 2, figsize=(12, 12))\n",
        "ax[0, 0].imshow(np.transpose(image0[0], (1, 2, 0)))\n",
        "ax[0, 1].imshow(np.transpose(image0[1], (1, 2, 0)))\n",
        "ax[1, 0].imshow(np.transpose(image1[0], (1, 2, 0)))\n",
        "ax[1, 1].imshow(np.transpose(image1[1], (1, 2, 0)))\n",
        "ax[2, 0].imshow(np.transpose(image2[0], (1, 2, 0)))\n",
        "ax[2, 1].imshow(np.transpose(image2[1], (1, 2, 0)))\n",
        "\n",
        "ax[0, 0].set_title(\"Training Images\");\n",
        "ax[0, 1].set_title(\"Ground Truth Images\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-EwxpqIm_pA"
      },
      "source": [
        "### 3.5 Setup Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxM0WVJ_m_pA"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = transformed_train_dataset, transformed_val_dataset, transformed_test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCfpkMELm_pA"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHK66gMQm_pB"
      },
      "source": [
        "# 4. GAN Model\n",
        "\n",
        "The special type of GANs we plan to use is an DCGANs, which is a variation of the deep learning models composed of two convolutional neural networks — a generator and a discriminator — trained in opposition. In image processing tasks like inpainting, super-resolution, or style transfer, DCGANs learn to generate realistic images by mimicking the distribution of real images.\n",
        "\n",
        "The generator aims to produce realistic images that can fool the discriminator. For image processing tasks like inpainting, the generator is conditioned to a masked input image with the center noise.\n",
        "\n",
        "In the end, the goal is to minimize the loss of our mode, which from [lec12](https://cs.uwaterloo.ca/~yboykov/Courses/cs484/Lectures/lec12_weak_supervision.pdf?r=0.299853984606967), we know to be:\n",
        "\n",
        "\n",
        "$$\n",
        "\\min_{w} \\max_{\\theta}\n",
        "\\left(\n",
        "- \\sum_{i \\in \\mathit{Real}} \\ln \\sigma_w(I_i)\n",
        "- \\sum_{i \\in \\mathit{Fake}} \\ln \\left(1 - \\sigma_w(I_{\\theta}(z_i)) \\right)\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "We experimented various architectures, referencing various academic papers for inspiration. We took inspiration from [S. Iizuka, E. Simo-Serra, H. Ishikawa](https://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf), modifying the achitecture to fit our needs. One notable difference we chose to not take from the paper was the use of 2 discriminators, a global one and local one. To simplify our training we instead had one regular discriminator that functions as a simple CNN to classify between authentic and generated images.  \n",
        "\n",
        "The generator uses an encoder-decoder structure with convolutional layers to extract spatial features and deconvolutional layers to reconstruct the full image. Dilated convolutions expand the receptive field, helping the model capture broader context for coherent inpainting. Batch normalization stabilizes training and improves convergence. The discriminator, designed as a patch-based classifier, uses convolution and downsampling to evaluate image realism, focusing on structural and textural consistency. This adversarial setup pushes the generator to create realistic, context-aware content. Together, these components enable effective learning of both local details and global patterns for high-quality image inpainting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNencbhjm_pB"
      },
      "source": [
        "### 4.1 Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXaQqNezm_pB"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        This is the generator class. The generator takes in a masked image and its job is to inpaint (fill in) the\n",
        "        missing part of the masked image.\n",
        "\n",
        "        The architecture of the generator is inspired by the generator in the article mentioned\n",
        "        earlier:\n",
        "            1) 4x64x64    -> 64x64x64\n",
        "            2) 64x64x64   -> 128x32x32\n",
        "            3) 128x32x32  -> 256x16x16\n",
        "            4) 256x16x16  -> 256x16x16\n",
        "            5) 256x16x16  -> 256x14x14\n",
        "            6) 256x14x14  -> 256x8x8\n",
        "            7) 256x8x8    -> 256x8x8\n",
        "            8) 256x8x8    -> 128x16x16\n",
        "            9) 128x16x16  -> 128x16x16\n",
        "            10) 128x16x16  -> 64x32x32\n",
        "            11) 64x32x32   -> 64x64x64\n",
        "            12) 64x64x64   -> 64x64x64\n",
        "            13) 64x64x64   -> 3x64x64\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, im_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(im_channels + 1, 64, 5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "\n",
        "        # Dilation\n",
        "        self.conv5 = nn.Conv2d(256, 256, 3, stride=1, padding=1, dilation=2)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, stride=1, padding=1, dilation=4)\n",
        "        self.conv7 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "\n",
        "        # Decoder\n",
        "        self.deconv8 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
        "        self.conv9 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
        "\n",
        "        self.deconv10 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
        "        self.deconv11 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.conv13 = nn.Conv2d(64, 3, 3, stride=1, padding=1)\n",
        "\n",
        "        # Batch norms for each layer\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.bn8 = nn.BatchNorm2d(128)\n",
        "        self.bn9 = nn.BatchNorm2d(128)\n",
        "        self.bn10 = nn.BatchNorm2d(64)\n",
        "        self.bn11 = nn.BatchNorm2d(64)\n",
        "        self.bn12 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Activation Functions\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder layers\n",
        "        x1 = self.bn1(self.relu(self.conv1(x)))\n",
        "        x2 = self.bn2(self.relu(self.conv2(x1)))\n",
        "        x3 = self.bn3(self.relu(self.conv3(x2)))\n",
        "        x4 = self.bn4(self.relu(self.conv4(x3)))\n",
        "\n",
        "        # Dilation Layers\n",
        "        x5 = self.bn5(self.relu(self.conv5(x4)))\n",
        "        x6 = self.bn6(self.relu(self.conv6(x5)))\n",
        "        x7 = self.bn7(self.relu(self.conv7(x6)))\n",
        "\n",
        "        # Decoder layers\n",
        "        x8 = self.bn8(self.relu(self.deconv8(x7)))\n",
        "        x9 = self.bn9(self.relu(self.conv9(x8)))\n",
        "        x10 = self.bn10(self.relu(self.deconv10(x9)))\n",
        "        x11 = self.bn11(self.relu(self.deconv11(x10)))\n",
        "        x12 = self.bn12(self.relu(self.conv12(x11)))\n",
        "\n",
        "        output = self.sig(self.conv13(x12))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdGYVnMom_pB"
      },
      "source": [
        "### 4.2 Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6hTSiwBm_pB"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "        This is the discriminator class. The discriminator takes in an image and its job is to classify whether the image is generated(fake)\n",
        "        or real (non-generated).\n",
        "\n",
        "        The architecture of the network is the same as a simple CNN classifier\n",
        "        with the following architecture:\n",
        "            1) 3x64x64   -> 64x32x32\n",
        "            2) 64x32x32  -> 128x16x16\n",
        "            3) 128x16X16 -> 256x8x8\n",
        "            4) 256x8x8   -> 512x4x4\n",
        "            5) 512x4x4   -> 512x1x1\n",
        "            6) 512 -> 512\n",
        "            7) 512 -> 512\n",
        "            8) 512 -> 256\n",
        "            9) 256 -> 10\n",
        "            10) 10 -> 1\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Batch norm layers\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Linear layers\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 10)\n",
        "        self.fc5 = nn.Linear(10, 1)\n",
        "\n",
        "        # Activation functions and pooling\n",
        "        self.down_sample = nn.AvgPool2d(2)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forwad pass through the layers\n",
        "        x = self.down_sample(self.conv1(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        x = self.down_sample(self.conv2(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x = self.down_sample(self.conv3(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        x = self.down_sample(self.conv4(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn4(x)\n",
        "\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Training"
      ],
      "metadata": {
        "id": "yuUQb4t5W07o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "406t2cSin0iD"
      },
      "outputs": [],
      "source": [
        "def train(g, d, train_loader, val_loader, criterion, optimizer_g, optimizer_d, epochs, verbose=False, show_images=False):\n",
        "    \"\"\"\n",
        "    Trains model for image inpainting.\n",
        "\n",
        "    Args:\n",
        "        g: The generator model.\n",
        "        d: The discriminator model.\n",
        "        train_loader: DataLoader for the training dataset.\n",
        "        val_loader: DataLoader for the validation dataset.\n",
        "        criterion: The loss function.\n",
        "        optimizer_g: Optimizer for the generator.\n",
        "        optimizer_d: Optimizer for the discriminator.\n",
        "        epochs: The number of training epochs.\n",
        "        verbose: If True, prints training progress.\n",
        "        show_images: If True, displays generated images during validation.\n",
        "    \"\"\"\n",
        "    # Total losses for epoch x batches\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    total_steps = len(train_loader)\n",
        "    val_iter = iter(val_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        g.train()\n",
        "        d.train()\n",
        "\n",
        "        # Average loss per batch\n",
        "        g_avg_loss_per_batch = 0\n",
        "        d_avg_loss_per_batch = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            im_masked, im_truth = batch\n",
        "\n",
        "            # Move data to GPU\n",
        "            im_masked = im_masked.to(device)\n",
        "            im_truth = im_truth.to(device)\n",
        "\n",
        "            # Add the masks as a separate channel\n",
        "            N = batch[0].shape[0]\n",
        "            masks = create_masks(N).to(device)\n",
        "            im_masked = torch.cat((im_masked, masks), dim=1)\n",
        "\n",
        "            # --- Train Discriminator ---\n",
        "            d.zero_grad()\n",
        "\n",
        "            # Train discriminator with ground truth images\n",
        "            outputs = d(im_truth).view(-1)\n",
        "            true_labels = torch.ones(N).to(device)\n",
        "            d_err = criterion(outputs, true_labels)\n",
        "\n",
        "            # Train discriminator with fake images\n",
        "            fake_im = g(im_masked)\n",
        "            inpainted = fake_im * masks[0] + im_masked[:, :3, :, :]\n",
        "            output = d(inpainted.detach()).view(-1)\n",
        "            fake_labels = torch.zeros(N).to(device)\n",
        "            d_err += criterion(output, fake_labels)\n",
        "\n",
        "            d_err.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # --- Train Generator ---\n",
        "            g.zero_grad()\n",
        "\n",
        "            fake_im = g(im_masked)\n",
        "            inpainted = fake_im * masks[0] + im_masked[:, :3, :, :]\n",
        "            outputs = d(inpainted).view(-1)\n",
        "\n",
        "            g_err = criterion(outputs, true_labels)\n",
        "            g_err.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            # Keep track of loss\n",
        "            g_avg_loss_per_batch += g_err.item()\n",
        "            d_avg_loss_per_batch += d_err.item()\n",
        "\n",
        "            g_losses.append(g_err.item())\n",
        "            d_losses.append(d_err.item())\n",
        "\n",
        "            if show_images and i == 0:\n",
        "                g.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_batch = next(val_iter)\n",
        "                    val_masked, val_truth = val_batch\n",
        "\n",
        "                    # Move validation data to GPU\n",
        "                    val_masked = val_masked.to(device)\n",
        "                    val_truth = val_truth.to(device)\n",
        "\n",
        "                    N = val_masked.shape[0]\n",
        "                    val_masks = create_masks(N).to(device)\n",
        "                    val_input = torch.cat((val_masked, val_masks), dim=1)\n",
        "\n",
        "                    # Inpainting\n",
        "                    fake_val = g(val_input)\n",
        "                    inpainted = val_masked * (1 - val_masks) + fake_val * val_masks\n",
        "\n",
        "                    # Unpainted\n",
        "                    unpainted = val_truth * (1 - val_masks)\n",
        "\n",
        "                    # Get first 4 samples\n",
        "                    idx = slice(0, 4)\n",
        "                    sample_masked = val_masked[idx]\n",
        "                    sample_fake = fake_val[idx]\n",
        "                    sample_inpainted = inpainted[idx]\n",
        "                    sample_truth = val_truth[idx]\n",
        "\n",
        "                    images = [sample_masked, sample_fake, sample_inpainted, sample_truth]\n",
        "                    labels = [\"Masked Input\", \"Generated Output\", \"Inpainted\", \"Ground Truth\"]\n",
        "\n",
        "                    # Plot\n",
        "                    fig, axs = plt.subplots(len(images), 4, figsize=(16, 10))\n",
        "                    for row in range(len(images)):\n",
        "                        for col in range(4):\n",
        "                            img = images[row][col].cpu()\n",
        "                            img = img.permute(1, 2, 0)  # CxHxW → HxWxC\n",
        "                            axs[row, col].imshow(img.numpy(), cmap='gray' if img.shape[2] == 1 else None)\n",
        "                            axs[row, col].axis('off')\n",
        "                            if col == 0:\n",
        "                                axs[row, col].set_title(labels[row], fontsize=10, loc='left')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                g.train()\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{total_steps}], '\n",
        "                    f'Discriminator Loss: {(d_avg_loss_per_batch/total_steps):.4f}, '\n",
        "                    f'Generator Loss: {(g_avg_loss_per_batch/total_steps):.4f}')\n",
        "\n",
        "    return g_losses, d_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first do a basic training of our model. We will observe how the generator fills in the masked hole after each epoch and compare it to the ground truth."
      ],
      "metadata": {
        "id": "Q8lPQXDww4kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = Generator(im_channels=3).to(device)\n",
        "d = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = optim.Adam(g.parameters(), lr=0.0001)\n",
        "optimizer_d = optim.Adam(d.parameters(), lr=0.0001)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "num_epochs = 10\n",
        "\n",
        "g_losses, d_losses = train(g, d, train_loader, val_loader, criterion, optimizer_g, optimizer_d, epochs=10, verbose=True, show_images=True)\n"
      ],
      "metadata": {
        "id": "F3V2NxAwn8_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(g_losses, label=\"G\")\n",
        "plt.plot(d_losses, label=\"D\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I3ZRbWynvpcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(g.state_dict(), f\"initial_generator.pt\")\n",
        "torch.save(d.state_dict(), f\"initial_discriminator.pt\")"
      ],
      "metadata": {
        "id": "38ZxiFeuu9oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Experiments"
      ],
      "metadata": {
        "id": "HJqd1N6Jn30Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trained_model_or_train(experiment_name, experiment_params):\n",
        "    model_dir = f\"models/{experiment_name}\"\n",
        "    if os.path.exists(model_dir):\n",
        "        print(f\"Found Experiment {experiment_name}\")\n",
        "        trained_generator = Generator(im_channels=3).to(device)\n",
        "        trained_discriminator = Discriminator().to(device)\n",
        "        trained_generator.load_state_dict(torch.load(f\"{model_dir}/generator.pt\"))\n",
        "        trained_discriminator.load_state_dict(torch.load(f\"{model_dir}/discriminator.pt\"))\n",
        "\n",
        "        # Load losses if they exist\n",
        "        g_losses_path = f\"{model_dir}/g_losses.pt\"\n",
        "        d_losses_path = f\"{model_dir}/d_losses.pt\"\n",
        "        g_losses = torch.load(g_losses_path) if os.path.exists(g_losses_path) else []\n",
        "        d_losses = torch.load(d_losses_path) if os.path.exists(d_losses_path) else []\n",
        "\n",
        "        return trained_generator, trained_discriminator, g_losses, d_losses\n",
        "    else:\n",
        "        generator = Generator(im_channels=3).to(device)\n",
        "        discriminator = Discriminator().to(device)\n",
        "\n",
        "        g_losses, d_losses = train(g=generator, d=discriminator, **experiment_params)\n",
        "        os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
        "        torch.save(generator.state_dict(), f\"{model_dir}/generator.pt\")\n",
        "        torch.save(discriminator.state_dict(), f\"{model_dir}/discriminator.pt\")\n",
        "\n",
        "        # Save losses\n",
        "        torch.save(g_losses, f\"{model_dir}/g_losses.pt\")\n",
        "        torch.save(d_losses, f\"{model_dir}/d_losses.pt\")\n",
        "\n",
        "        return generator, discriminator, g_losses, d_losses"
      ],
      "metadata": {
        "id": "B-GZVApJw11N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 L1 Loss"
      ],
      "metadata": {
        "id": "urdIUvmQ-k2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_g = optim.Adam(g.parameters(), lr=0.0001)\n",
        "optimizer_d = optim.Adam(d.parameters(), lr=0.0001)\n",
        "criterion = nn.L1Loss()\n",
        "num_epochs = 10\n",
        "\n",
        "g, d, g_losses, d_losses = get_trained_model_or_train(\n",
        "    experiment_name=\"l1_loss\",\n",
        "    experiment_params={\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"criterion\": criterion,\n",
        "        \"optimizer_g\": optimizer_g,\n",
        "        \"optimizer_d\": optimizer_d,\n",
        "        \"epochs\": num_epochs,\n",
        "        \"verbose\": True,\n",
        "        \"show_images\": False\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "X379mFzf-n95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 L2 (MSE) Loss\n",
        "MSELoss and L1Loss not seemed to work with our model?? Could just scrap later"
      ],
      "metadata": {
        "id": "037tqVeF1HFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_g = optim.Adam(g.parameters(), lr=0.0001)\n",
        "optimizer_d = optim.Adam(d.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "num_epochs = 10\n",
        "\n",
        "g, d, g_losses, d_losses = get_trained_model_or_train(\n",
        "    experiment_name=\"l1_loss\",\n",
        "    experiment_params={\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"criterion\": criterion,\n",
        "        \"optimizer_g\": optimizer_g,\n",
        "        \"optimizer_d\": optimizer_d,\n",
        "        \"epochs\": num_epochs,\n",
        "        \"verbose\": True,\n",
        "        \"show_images\": False\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ARfoYSP6n1rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 Huber loss TODO???\n",
        "\n"
      ],
      "metadata": {
        "id": "46u_HaTN-ziJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_g = optim.Adam(g.parameters(), lr=0.0001)\n",
        "optimizer_d = optim.Adam(d.parameters(), lr=0.0001)\n",
        "criterion = nn.HuberLoss()\n",
        "num_epochs = 10\n",
        "\n",
        "g, d, g_losses, d_losses = get_trained_model_or_train(\n",
        "    experiment_name=\"l1_loss\",\n",
        "    experiment_params={\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"criterion\": criterion,\n",
        "        \"optimizer_g\": optimizer_g,\n",
        "        \"optimizer_d\": optimizer_d,\n",
        "        \"epochs\": num_epochs,\n",
        "        \"verbose\": True,\n",
        "        \"show_images\": False\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "_VfOEvxr-6-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained models\n",
        "g = Generator(im_channels=3).to(device)\n",
        "d = Discriminator().to(device)\n",
        "\n",
        "g.load_state_dict(torch.load(\"initial_generator.pt\", map_location=device))\n",
        "d.load_state_dict(torch.load(\"initial_discriminator.pt\", map_location=device))\n"
      ],
      "metadata": {
        "id": "S5rTpxoa_hdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows results after training\n",
        "num_evaluation = 10\n",
        "num_image_display = 5\n",
        "val_iter = iter(val_loader)\n",
        "\n",
        "# Loss\n",
        "g_tot_loss = 0\n",
        "d_tot_loss = 0\n",
        "num_total = 1\n",
        "\n",
        "# Validate GAN\n",
        "f, ax = plt.subplots(num_image_display, 2, figsize=(12, 12))\n",
        "ax[0, 0].set_title(\"Inpainted Images (Fake)\")\n",
        "ax[0, 1].set_title(\"Ground Truth Images\")\n",
        "for i in range(num_evaluation):\n",
        "  g.eval()\n",
        "  d.eval()\n",
        "  with torch.no_grad():\n",
        "      val_masked, val_truth = val_batch\n",
        "      val_batch = next(val_iter)\n",
        "      N = len(val_masked)\n",
        "\n",
        "      # Move validation data to GPU\n",
        "      val_masked = val_masked.to(device)\n",
        "      val_truth = val_truth.to(device)\n",
        "      N = len(val_masked)\n",
        "\n",
        "      # Get labels\n",
        "      true_labels = torch.ones(N).to(device)\n",
        "      fake_labels = torch.zeros(N).to(device)\n",
        "\n",
        "      val_masks = create_masks(N).to(device)\n",
        "      val_masked = torch.cat((val_masked, val_masks), dim=1)\n",
        "\n",
        "      # Get the inpainted image using the generator\n",
        "      fake_im = g(val_masked)\n",
        "      inpainted = fake_im * val_masks[0] + val_masked[:, :3, :, :]\n",
        "      outputs = d(inpainted).view(-1)\n",
        "\n",
        "      d_tot_loss += criterion(outputs, fake_labels).item()\n",
        "      g_tot_loss += criterion(outputs, true_labels).item()\n",
        "      num_total += 1\n",
        "\n",
        "      if (i < num_image_display):\n",
        "        # Display first sample\n",
        "        ax[i, 0].imshow(inpainted[0].cpu().permute(1, 2, 0))\n",
        "        ax[i, 1].imshow(val_truth[0].cpu().permute(1, 2, 0))\n",
        "\n",
        "plt.show()\n",
        "print(f'Number of Validation Images: {num_total * N}, '\n",
        "            f'Average Discriminator Loss: {d_tot_loss / num_total:.4f}, '\n",
        "            f'Average Generator Loss: {d_tot_loss / num_total:.4f}')"
      ],
      "metadata": {
        "id": "bpDC6-J0SVu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conclusions\n",
        "\n",
        "Our cDCGAN model demonstrated promising results in the object removal and inpainting task. It was able to generate visually plausible completions of the missing regions, especially for relatively uniform backgrounds like sky or fur. However, the model still has clear room for improvement. Recent advances such as Stable Diffusion and other Transformer-based architectures have shown significant gains in generative tasks. Incorporating these state-of-the-art models, though computationally expensive, could potentially lead to more coherent and higher-quality results.\n",
        "\n",
        "At the start of our project, we simplified the problem by always masking a fixed square at the center of each image. This made the task more controlled, but less practical for real-world applications. A more flexible approach would involve user-defined masks of arbitrary shape and size. Supporting custom masks would make the model far more robust and widely usable across different tasks. Future work could involve incorporating spatial attention or segmentation cues to guide such inpainting.\n",
        "\n",
        "Training the model involved experimenting with several loss functions, including L1, L2, and Huber loss, which are commonly used for regression tasks. However, these losses did not yield stable or visually pleasing outputs in our setting. We eventually settled on binary cross-entropy (BCE) loss due to its alignment with the adversarial training objective and improved qualitative results. Still, finding the right balance between adversarial loss and reconstruction loss remains a challenge. Future exploration into hybrid loss strategies or perceptual loss may help improve output fidelity.\n",
        "\n",
        "One significant challenge we encountered was the inconsistency in our dataset of cat images. Many images were not standardized—cats varied in position, lighting, and scale, and some images even included human hands or other distractions. These inconsistencies made it harder for the model to learn a clean background distribution. Moving forward, it may be beneficial to either cleanse the current dataset or switch to a more standardized and curated one. Starting with an easier dataset may also allow the model to learn more effectively before tackling more complex scenarios."
      ],
      "metadata": {
        "id": "HSXoU9DX7f2r"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}